{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WAPiZjOBSbzh"
      },
      "source": [
        "# ==========================================\n",
        "# Sanvia - 3 - Feature Extraction & Report Generation\n",
        "# =========================================="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rDPVuetmSoBw"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "import requests\n",
        "import base64\n",
        "from io import BytesIO\n",
        "from PIL import Image\n",
        "import sys\n",
        "sys.path.append(\"/content/drive/MyDrive/models\")\n",
        "\n",
        "from sanvia_layers import (\n",
        "    EfficientCrossAttention,\n",
        "    TabularEncoder,\n",
        "    GatedFusionLayer,\n",
        "    FocalLoss\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUA9802cSfvA"
      },
      "source": [
        "# ==========================================\n",
        "# 1. ENVIRONMENT SETUP\n",
        "# ==========================================\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XXS2sQlBSffA"
      },
      "outputs": [],
      "source": [
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "# API Configuration - Change these to your actual API credentials\n",
        "OPENAI_API_KEY = ##\"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\"\n",
        "\n",
        "\n",
        "# Use OPENAI or GEMINI\n",
        "VLM_API_PROVIDER = \"openai\"  # ÿ£Ÿà \"gemini\"\n",
        "\n",
        "DATA_DIR = Path('/content/drive/MyDrive/VnDir_Mammo')\n",
        "OUTPUT_DIR = DATA_DIR / 'sanvia_outputs'\n",
        "CONFIG_PATH = OUTPUT_DIR / 'config.json'\n",
        "MODEL_PATH = OUTPUT_DIR / 'best_sanvia_model.h5'\n",
        "VIEW_MAPPING_PATH = OUTPUT_DIR / 'view_mapping_final.csv'\n",
        "IMAGE_CACHE_PATH = OUTPUT_DIR / 'image_paths_cache.json'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tl2ePkkUSqbB"
      },
      "source": [
        "# ==========================================\n",
        "# 2. LOAD CONFIGURATION & TRAINED MODEL\n",
        "# =========================================="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "yXaCC42uS5IR",
        "outputId": "bdb37856-64aa-45cb-f47e-3c8d1857ed97"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'str' object has no attribute 'exists'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1769969201.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0msanvia_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_trained_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODEL_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-1769969201.py\u001b[0m in \u001b[0;36mload_trained_model\u001b[0;34m(model_path)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_trained_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Model not found at {model_path}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"‚úÖ Model loaded: {model.name}, Layers: {len(model.layers)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'exists'"
          ]
        }
      ],
      "source": [
        "# Load configuration\n",
        "with open(CONFIG_PATH, 'r') as f:\n",
        "    CONFIG = json.load(f)\n",
        "\n",
        "# CRITICAL FIX: Convert string paths back to Path objects after JSON loading\n",
        "CONFIG['data_dir'] = Path(CONFIG['data_dir'])\n",
        "CONFIG['images_dir'] = Path(CONFIG['images_dir'])\n",
        "CONFIG['output_dir'] = Path(CONFIG['output_dir'])\n",
        "if 'scaler_path' in CONFIG:\n",
        "    CONFIG['scaler_path'] = Path(CONFIG['scaler_path'])\n",
        "if 'class_weights_path' in CONFIG:\n",
        "    CONFIG['class_weights_path'] = Path(CONFIG['class_weights_path'])\n",
        "\n",
        "# Load view mapping\n",
        "view_mapping_df = pd.read_csv(VIEW_MAPPING_PATH)\n",
        "\n",
        "# Load image cache\n",
        "with open(IMAGE_CACHE_PATH, 'r') as f:\n",
        "    IMAGE_PATH_CACHE = json.load(f)\n",
        "\n",
        "print(\"‚úÖ Configuration and data loaded successfully\")\n",
        "# Load the trained model from notebook 2\n",
        "def load_trained_model(model_path: Path):\n",
        "    \"\"\"Load the complete trained model directly without rebuilding\"\"\"\n",
        "    if not model_path.exists():\n",
        "        raise FileNotFoundError(f\"Model not found at {model_path}\")\n",
        "\n",
        "    # ÿ™ÿ≠ŸÖŸäŸÑ ÿßŸÑŸÖŸàÿØŸÑ ÿßŸÑŸÉÿßŸÖŸÑ ŸÖÿ®ÿßÿ¥ÿ±ÿ©\n",
        "    sanvia_model = tf.keras.models.load_model(str(model_path))\n",
        "    print(f\"‚úÖ Complete model loaded from: {model_path}\")\n",
        "    print(f\"   - Model name: {sanvia_model.name}\")\n",
        "    print(f\"   - Number of layers: {len(sanvia_model.layers)}\")\n",
        "    return sanvia_model\n",
        "\n",
        "# ÿßÿ≥ÿ™ÿÆÿØÿßŸÖ ÿßŸÑŸÖÿ≥ÿßÿ± ÿßŸÑÿµÿ≠Ÿäÿ≠ ÿßŸÑÿ∞Ÿä ÿ≠ŸÅÿ∏ÿ™ ÿ®Ÿá\n",
        "MODEL_PATH = Path('/content/drive/MyDrive/models/Sanvia.keras')  # ÿ£Ÿà .h5\n",
        "\n",
        "# ÿ™ÿ≠ŸÖŸäŸÑ ÿßŸÑŸÖŸàÿØŸÑ ŸÖÿ®ÿßÿ¥ÿ±ÿ© ÿ®ÿØŸàŸÜ ÿ•ÿπÿßÿØÿ© ÿ®ŸÜÿßÿ°\n",
        "sanvia_model = load_trained_model(MODEL_PATH)\n",
        "\n",
        "print(f\"‚úÖ Model loaded successfully!\")\n",
        "print(f\"   - Total parameters: {sanvia_model.count_params():,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dxIqOJ-GQTw8"
      },
      "outputs": [],
      "source": [
        "CONFIG.update({\n",
        "    'img_size': (768, 768),\n",
        "})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldjFJdjVS7ho"
      },
      "source": [
        "# ==========================================\n",
        "# 3. FEATURE EXTRACTION PIPELINE\n",
        "# =========================================="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZWcoPFrxS9_x"
      },
      "outputs": [],
      "source": [
        "def extract_features_from_study(study_id: str, view_df: pd.DataFrame, model_path: Path) -> Dict:\n",
        "    \"\"\"\n",
        "    Extract deep features using a clean approach\n",
        "    \"\"\"\n",
        "    # Load the model FRESH inside this function to avoid state issues\n",
        "    model = tf.keras.models.load_model(\n",
        "        str(model_path),\n",
        "        custom_objects={\n",
        "            'TabularEncoder': type('TabularEncoder', (tf.keras.layers.Layer,), {}),\n",
        "            'EfficientCrossAttention': type('EfficientCrossAttention', (tf.keras.layers.Layer,), {}),\n",
        "            'GatedFusionLayer': type('GatedFusionLayer', (tf.keras.layers.Layer,), {})\n",
        "        },\n",
        "        compile=False\n",
        "    )\n",
        "\n",
        "    study_data = view_df[view_df['study_id'] == study_id].iloc[0]\n",
        "\n",
        "    # Load and preprocess images\n",
        "    image_arrays = []\n",
        "    for view in ['L_CC', 'L_MLO', 'R_CC', 'R_MLO']:\n",
        "        img_id = study_data[f'image_id_{view}']\n",
        "        if pd.isna(img_id) or img_id == 'missing.png':\n",
        "            img_path = str(CONFIG['images_dir'] / \"missing.png\")\n",
        "        else:\n",
        "            if not img_id.endswith('.png'):\n",
        "                img_id += '.png'\n",
        "            img_path = IMAGE_PATH_CACHE.get(img_id, str(CONFIG['images_dir'] / img_id))\n",
        "\n",
        "        # Load and preprocess\n",
        "        img = tf.io.read_file(img_path)\n",
        "        img = tf.io.decode_png(img, channels=CONFIG['num_channels'])\n",
        "        img = tf.image.convert_image_dtype(img, tf.float32)\n",
        "        img = tf.image.resize_with_pad(img, CONFIG['img_size'][0], CONFIG['img_size'][1])\n",
        "        image_arrays.append(img)\n",
        "\n",
        "    # Prepare tabular data\n",
        "    tabular = tf.stack([\n",
        "        float(study_data['age_norm']),\n",
        "        float(study_data['age_missing_flag'])\n",
        "    ], axis=0)[tf.newaxis, ...]\n",
        "\n",
        "    # Create input list (order matters - same as model.input order)\n",
        "    # Get the input names in order\n",
        "    input_names = [inp.name.split(':')[0] for inp in model.input]\n",
        "\n",
        "    # Create input dictionary\n",
        "    inputs_dict = {\n",
        "        'L_CC': image_arrays[0][tf.newaxis, ...],\n",
        "        'L_MLO': image_arrays[1][tf.newaxis, ...],\n",
        "        'R_CC': image_arrays[2][tf.newaxis, ...],\n",
        "        'R_MLO': image_arrays[3][tf.newaxis, ...],\n",
        "        'tabular': tabular\n",
        "    }\n",
        "\n",
        "    # Get predictions (this should work now)\n",
        "    try:\n",
        "        predictions = model(inputs_dict)\n",
        "        print(\"‚úÖ Model prediction successful\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Model prediction failed: {e}\")\n",
        "        # Try alternative approach\n",
        "        inputs_list = [\n",
        "            image_arrays[0][tf.newaxis, ...],\n",
        "            image_arrays[1][tf.newaxis, ...],\n",
        "            image_arrays[2][tf.newaxis, ...],\n",
        "            image_arrays[3][tf.newaxis, ...],\n",
        "            tabular\n",
        "        ]\n",
        "        predictions = model(inputs_list)\n",
        "\n",
        "    # Create feature extractor\n",
        "    layer_names = ['efficientnetb4', 'dense_10', 'dense_11']\n",
        "    outputs = []\n",
        "    for name in layer_names:\n",
        "        try:\n",
        "            layer = model.get_layer(name)\n",
        "            outputs.append(layer.output)\n",
        "        except:\n",
        "            print(f\"Warning: Layer {name} not found\")\n",
        "            continue\n",
        "\n",
        "    if len(outputs) == 3:\n",
        "        feature_extractor = tf.keras.Model(\n",
        "            inputs=model.input,\n",
        "            outputs=outputs\n",
        "        )\n",
        "\n",
        "        # Extract features\n",
        "        features = feature_extractor(inputs_dict)\n",
        "    else:\n",
        "        # Fallback - use predictions as features\n",
        "        features = [predictions[0], predictions[1]]\n",
        "\n",
        "    return {\n",
        "        'study_id': study_id,\n",
        "        'visual_features': features[0].numpy() if hasattr(features[0], 'numpy') else features[0],\n",
        "        'birads_features': features[1].numpy() if hasattr(features[1], 'numpy') else features[1],\n",
        "        'density_features': features[2].numpy() if len(features) > 2 else features[1],\n",
        "        'tabular_data': tabular.numpy(),\n",
        "        'predictions': {\n",
        "            'birads': predictions[0].numpy(),\n",
        "            'density': predictions[1].numpy()\n",
        "        }\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MOjGEGEbTACw"
      },
      "source": [
        "# ==========================================\n",
        "# 4. EXPLAINABLE AI - GRAD-CAM\n",
        "# =========================================="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fuaA4zRkS_sx"
      },
      "outputs": [],
      "source": [
        "def generate_gradcam_heatmap(model, img_array, layer_name='efficientnetb4'):\n",
        "    \"\"\"\n",
        "    Generate Grad-CAM heatmap for visualization\n",
        "    \"\"\"\n",
        "    # Get the last convolutional layer from EfficientNet\n",
        "    last_conv_layer = model.get_layer(layer_name)\n",
        "\n",
        "    # Create a model that maps the input image to the activations of the last conv layer\n",
        "    # and the output predictions\n",
        "    grad_model = tf.keras.Model(\n",
        "        [model.inputs],\n",
        "        [last_conv_layer.output, model.output]\n",
        "    )\n",
        "\n",
        "    # Record gradients\n",
        "    with tf.GradientTape() as tape:\n",
        "        conv_outputs, predictions = grad_model([img_array])\n",
        "        # Use birads prediction\n",
        "        loss = predictions[0][:, tf.argmax(predictions[0][0])]\n",
        "\n",
        "    # Extract gradients\n",
        "    grads = tape.gradient(loss, conv_outputs)\n",
        "\n",
        "    # Global average pooling of gradients\n",
        "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
        "\n",
        "    # Weighted combination of feature maps\n",
        "    conv_outputs = conv_outputs[0]\n",
        "    heatmap = tf.reduce_mean(\n",
        "        tf.multiply(pooled_grads, conv_outputs),\n",
        "        axis=-1\n",
        "    )\n",
        "\n",
        "    # Normalize heatmap\n",
        "    heatmap = np.maximum(heatmap, 0)\n",
        "    heatmap /= np.max(heatmap)\n",
        "\n",
        "    return heatmap.numpy()\n",
        "\n",
        "def overlay_heatmap_on_image(img_path, heatmap, alpha=0.4):\n",
        "    \"\"\"\n",
        "    Overlay heatmap on original image for visualization\n",
        "    \"\"\"\n",
        "    # Load original image\n",
        "    img = cv2.imread(img_path)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    img = cv2.resize(img, CONFIG['img_size'])\n",
        "\n",
        "    # Resize heatmap\n",
        "    heatmap = cv2.resize(heatmap, CONFIG['img_size'])\n",
        "    heatmap = np.uint8(255 * heatmap)\n",
        "\n",
        "    # Apply colormap\n",
        "    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
        "    heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Overlay\n",
        "    superimposed_img = heatmap * alpha + img * (1 - alpha)\n",
        "    superimposed_img = np.uint8(superimposed_img)\n",
        "\n",
        "    return superimposed_img"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5z5eNkSbTN0p"
      },
      "source": [
        "# ==========================================\n",
        "# 5. VISION-LANGUAGE MODEL API INTEGRATION\n",
        "# =========================================="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "vZ4_K9exTLvR"
      },
      "outputs": [],
      "source": [
        "def encode_image_to_base64(image_path: str) -> str:\n",
        "    \"\"\"Convert image to base64 string for API transmission\"\"\"\n",
        "    with open(image_path, \"rb\") as image_file:\n",
        "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
        "\n",
        "def prepare_images_for_vlm(study_id: str, view_df: pd.DataFrame) -> Dict[str, str]:\n",
        "    \"\"\"\n",
        "    Prepare all four views encoded as base64 for API transmission\n",
        "    \"\"\"\n",
        "    study_data = view_df[view_df['study_id'] == study_id].iloc[0]\n",
        "    encoded_images = {}\n",
        "\n",
        "    for view in ['L_CC', 'L_MLO', 'R_CC', 'R_MLO']:\n",
        "        img_id = study_data[f'image_id_{view}']\n",
        "        if pd.isna(img_id) or img_id == 'missing.png':\n",
        "            img_path = str(CONFIG['images_dir'] / \"missing.png\")\n",
        "        else:\n",
        "            if not img_id.endswith('.png'):\n",
        "                img_id += '.png'\n",
        "            img_path = IMAGE_PATH_CACHE.get(img_id, str(CONFIG['images_dir'] / img_id))\n",
        "\n",
        "        encoded_images[view] = encode_image_to_base64(img_path)\n",
        "\n",
        "    return encoded_images\n",
        "\n",
        "def generate_medical_report_via_api(\n",
        "    study_id: str,\n",
        "    view_df: pd.DataFrame,\n",
        "    features: Dict,\n",
        "    api_provider: str = VLM_API_PROVIDER\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Generate medical report using Vision-Language Model API\n",
        "    \"\"\"\n",
        "    study_data = view_df[view_df['study_id'] == study_id].iloc[0]\n",
        "\n",
        "    # Extract predictions\n",
        "    birads_pred = np.argmax(features['predictions']['birads'][0])\n",
        "    density_pred = np.argmax(features['predictions']['density'][0])\n",
        "\n",
        "    # Convert to actual BI-RADS scale (1-5)\n",
        "    birads_class = birads_pred + 1\n",
        "    density_map = {0: 'A', 1: 'B', 2: 'C', 3: 'D'}\n",
        "    density_class = density_map.get(density_pred, 'Unknown')\n",
        "\n",
        "    # Prepare clinical data text\n",
        "    clinical_info = f\"\"\"\n",
        "    Patient Age: {int(study_data.get('age', 45)) if not pd.isna(study_data.get('age')) else 'Not specified'}\n",
        "    Laterality: Left and Right Breast\n",
        "    Views: CC and MLO\n",
        "    Predicted BI-RADS Category: {birads_class}\n",
        "    Predicted Breast Density: Category {density_class}\n",
        "    \"\"\"\n",
        "\n",
        "    # Prepare VLM prompt\n",
        "    prompt = f\"\"\"\n",
        "    You are a radiologist assistant analyzing mammogram images. Based on the provided mammogram images (CC and MLO views for both left and right breasts) and the following clinical predictions, generate a structured medical report.\n",
        "\n",
        "    CLINICAL PREDICTIONS:\n",
        "    {clinical_info}\n",
        "\n",
        "    Please provide:\n",
        "    1. Overall impression\n",
        "    2. Key findings (if any abnormalities detected)\n",
        "    3. BI-RADS classification justification\n",
        "    4. Recommendations for follow-up\n",
        "\n",
        "    Report should be professional, concise, and suitable for clinical documentation.\n",
        "    \"\"\"\n",
        "\n",
        "    # Get encoded images\n",
        "    encoded_images = prepare_images_for_vlm(study_id, view_df)\n",
        "\n",
        "    if api_provider == \"openai\":\n",
        "        return _call_openai_api(prompt, encoded_images)\n",
        "    elif api_provider == \"gemini\":\n",
        "        return _call_gemini_api(prompt, encoded_images)\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported API provider: {api_provider}\")\n",
        "\n",
        "def _call_openai_api(prompt: str, encoded_images: Dict[str, str]) -> str:\n",
        "    \"\"\"\n",
        "    Call OpenAI GPT-4 Vision API\n",
        "    \"\"\"\n",
        "    headers = {\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"Authorization\": f\"Bearer {OPENAI_API_KEY}\"\n",
        "    }\n",
        "\n",
        "    # Prepare messages with images\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\n",
        "                    \"type\": \"text\",\n",
        "                    \"text\": prompt\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    # Add all four views as separate images\n",
        "    for view_name, encoded_image in encoded_images.items():\n",
        "        image_content = {\n",
        "            \"type\": \"image_url\",\n",
        "            \"image_url\": {\n",
        "                \"url\": f\"data:image/png;base64,{encoded_image}\",\n",
        "                \"detail\": \"high\"\n",
        "            }\n",
        "        }\n",
        "        messages[0]['content'].append(image_content)\n",
        "\n",
        "    payload = {\n",
        "        \"model\": \"gpt-4-vision-preview\",\n",
        "        \"messages\": messages,\n",
        "        \"max_tokens\": 1000,\n",
        "        \"temperature\": 0.3  # Low temperature for consistent medical reports\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.post(\n",
        "            \"https://api.openai.com/v1/chat/completions\",\n",
        "            headers=headers,\n",
        "            json=payload,\n",
        "            timeout=60\n",
        "        )\n",
        "        response.raise_for_status()\n",
        "        return response.json()['choices'][0]['message']['content']\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå API Error: {e}\")\n",
        "        return f\"Error generating report: {str(e)}\"\n",
        "\n",
        "def _call_gemini_api(prompt: str, encoded_images: Dict[str, str]) -> str:\n",
        "    \"\"\"\n",
        "    Call Google Gemini Vision API\n",
        "    \"\"\"\n",
        "    headers = {\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "\n",
        "    # Prepare parts with images\n",
        "    parts = [{\"text\": prompt}]\n",
        "    for view_name, encoded_image in encoded_images.items():\n",
        "        parts.append({\n",
        "            \"inline_data\": {\n",
        "                \"mime_type\": \"image/png\",\n",
        "                \"data\": encoded_image\n",
        "            }\n",
        "        })\n",
        "\n",
        "    payload = {\n",
        "        \"contents\": [{\n",
        "            \"parts\": parts\n",
        "        }],\n",
        "        \"generation_config\": {\n",
        "            \"maxOutputTokens\": 1000,\n",
        "            \"temperature\": 0.3\n",
        "        }\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.post(\n",
        "            f\"https://generativelanguage.googleapis.com/v1beta/models/gemini-pro-vision:generateContent?key={GEMINI_API_KEY}\",\n",
        "            headers=headers,\n",
        "            json=payload,\n",
        "            timeout=60\n",
        "        )\n",
        "        response.raise_for_status()\n",
        "        return response.json()['candidates'][0]['content']['parts'][0]['text']\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå API Error: {e}\")\n",
        "        return f\"Error generating report: {str(e)}\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n47INu9LTcIC"
      },
      "source": [
        "# ==========================================\n",
        "# 6. COMPREHENSIVE REPORT GENERATION\n",
        "# =========================================="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dX87c-woTgAx"
      },
      "outputs": [],
      "source": [
        "def generate_comprehensive_report(\n",
        "    study_id: str,\n",
        "    view_df: pd.DataFrame,\n",
        "    model,\n",
        "    include_xai: bool = True,\n",
        "    save_report: bool = True\n",
        ") -> Dict:\n",
        "    \"\"\"\n",
        "    Generate complete medical report with features, predictions, XAI visualizations, and VLM report\n",
        "    \"\"\"\n",
        "    print(f\"\\n\" + \"=\"*60)\n",
        "    print(f\"Generating Report for Study: {study_id}\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Step 1: Extract features\n",
        "    print(\"üìä Step 1: Extracting deep features...\")\n",
        "    features = extract_features_from_study(study_id, view_df, model)\n",
        "\n",
        "    # Step 2: Generate XAI visualizations\n",
        "    xai_images = {}\n",
        "    if include_xai:\n",
        "        print(\"üî• Step 2: Generating XAI heatmaps...\")\n",
        "        study_data = view_df[view_df['study_id'] == study_id].iloc[0]\n",
        "        for view in ['L_CC', 'L_MLO', 'R_CC', 'R_MLO']:\n",
        "            img_id = study_data[f'image_id_{view}']\n",
        "            if pd.isna(img_id) or img_id == 'missing.png':\n",
        "                continue\n",
        "\n",
        "            if not img_id.endswith('.png'):\n",
        "                img_id += '.png'\n",
        "            img_path = IMAGE_PATH_CACHE.get(img_id, str(CONFIG['images_dir'] / img_id))\n",
        "\n",
        "            # Generate heatmap\n",
        "            heatmap = generate_gradcam_heatmap(\n",
        "                model,\n",
        "                [img[tf.newaxis, ...] for img in [tf.image.resize_with_pad(\n",
        "                    tf.image.convert_image_dtype(\n",
        "                        tf.io.decode_png(tf.io.read_file(img_path), channels=3),\n",
        "                        tf.float32\n",
        "                    ),\n",
        "                    CONFIG['img_size'][0], CONFIG['img_size'][1]\n",
        "                ) for _ in range(4)]][0],  # Simplified for brevity\n",
        "                'efficientnetb4'\n",
        "            )\n",
        "\n",
        "            # Overlay on image\n",
        "            overlay_img = overlay_heatmap_on_image(img_path, heatmap)\n",
        "            xai_images[view] = overlay_img\n",
        "\n",
        "    # Step 3: Generate VLM report\n",
        "    print(\"ü§ñ Step 3: Generating AI report via VLM API...\")\n",
        "    vlm_report = generate_medical_report_via_api(study_id, view_df, features)\n",
        "\n",
        "    # Step 4: Compile comprehensive report\n",
        "    print(\"üìÑ Step 4: Compiling final report...\")\n",
        "\n",
        "    # Extract predictions\n",
        "    birads_pred = np.argmax(features['predictions']['birads'][0]) + 1\n",
        "    density_pred = np.argmax(features['predictions']['density'][0])\n",
        "    density_map = {0: 'A', 1: 'B', 2: 'C', 3: 'D'}\n",
        "\n",
        "    report = {\n",
        "        'study_id': study_id,\n",
        "        'timestamp': datetime.now().isoformat(),\n",
        "        'patient_info': {\n",
        "            'age': study_data.get('age', 'Not specified'),\n",
        "        },\n",
        "        'clinical_predictions': {\n",
        "            'bi_rads_category': birads_pred,\n",
        "            'breast_density': density_map.get(density_pred, 'Unknown'),\n",
        "            'confidence_scores': {\n",
        "                'bi_rads': float(np.max(features['predictions']['birads'][0])),\n",
        "                'density': float(np.max(features['predictions']['density'][0]))\n",
        "            }\n",
        "        },\n",
        "        'xai_visualizations': xai_images if include_xai else None,\n",
        "        'ai_generated_report': vlm_report,\n",
        "        'feature_vectors': {\n",
        "            'visual_features_shape': features['visual_features'].shape,\n",
        "            'birads_features_shape': features['birads_features'].shape,\n",
        "            'density_features_shape': features['density_features'].shape\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Step 5: Save report\n",
        "    if save_report:\n",
        "        save_report_to_file(report)\n",
        "\n",
        "    print(\"‚úÖ Report generation completed successfully!\")\n",
        "    return report\n",
        "\n",
        "def save_report_to_file(report: Dict, format: str = 'pdf'):\n",
        "    \"\"\"\n",
        "    Save the comprehensive report to file (Word/PDF)\n",
        "    \"\"\"\n",
        "    import matplotlib.pyplot as plt\n",
        "    from matplotlib.backends.backend_pdf import PdfPages\n",
        "\n",
        "    report_dir = OUTPUT_DIR / 'generated_reports'\n",
        "    report_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    report_path = report_dir / f\"report_{report['study_id']}_{timestamp}.pdf\"\n",
        "\n",
        "    with PdfPages(report_path) as pdf:\n",
        "        # Page 1: Summary\n",
        "        fig, ax = plt.subplots(figsize=(8.5, 11))\n",
        "        ax.axis('off')\n",
        "\n",
        "        report_text = f\"\"\"\n",
        "        MAMMOGRAPHY AI REPORT\n",
        "        =====================\n",
        "\n",
        "        Study ID: {report['study_id']}\n",
        "        Generated: {report['timestamp']}\n",
        "\n",
        "        PATIENT INFORMATION:\n",
        "        - Age: {report['patient_info']['age']}\n",
        "\n",
        "        AI PREDICTIONS:\n",
        "        - BI-RADS Category: {report['clinical_predictions']['bi_rads_category']}\n",
        "        - Breast Density: Category {report['clinical_predictions']['breast_density']}\n",
        "        - BI-RADS Confidence: {report['clinical_predictions']['confidence_scores']['bi_rads']:.2%}\n",
        "        - Density Confidence: {report['clinical_predictions']['confidence_scores']['density']:.2%}\n",
        "\n",
        "        AI GENERATED REPORT:\n",
        "        --------------------\n",
        "        {report['ai_generated_report']}\n",
        "        \"\"\"\n",
        "\n",
        "        ax.text(0.05, 0.95, report_text, transform=ax.transAxes,\n",
        "                fontsize=11, verticalalignment='top', fontfamily='monospace')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        pdf.savefig(fig)\n",
        "        plt.close()\n",
        "\n",
        "        # Pages 2-3: XAI Visualizations\n",
        "        if report['xai_visualizations']:\n",
        "            for view, overlay_img in report['xai_visualizations'].items():\n",
        "                fig, ax = plt.subplots(figsize=(8.5, 11))\n",
        "                ax.imshow(overlay_img)\n",
        "                ax.set_title(f'XAI Heatmap - {view} View', fontsize=14, pad=20)\n",
        "                ax.axis('off')\n",
        "                plt.tight_layout()\n",
        "                pdf.savefig(fig)\n",
        "                plt.close()\n",
        "\n",
        "    print(f\"üìÑ Report saved to: {report_path}\")\n",
        "    return report_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mYjonriTmYD"
      },
      "source": [
        "# ==========================================\n",
        "# 7. BATCH REPORT GENERATION\n",
        "# =========================================="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XY_gJxhGTkzh"
      },
      "outputs": [],
      "source": [
        "def generate_reports_for_studies(\n",
        "    study_ids: List[str],\n",
        "    view_df: pd.DataFrame,\n",
        "    model,\n",
        "    max_reports: int = 10\n",
        ") -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Generate reports for multiple studies\n",
        "    \"\"\"\n",
        "    reports = []\n",
        "\n",
        "    for i, study_id in enumerate(study_ids[:max_reports]):\n",
        "        print(f\"\\nProcessing study {i+1}/{len(study_ids[:max_reports])}\")\n",
        "        try:\n",
        "            report = generate_comprehensive_report(study_id, view_df, model)\n",
        "            reports.append(report)\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error processing study {study_id}: {e}\")\n",
        "            continue\n",
        "\n",
        "    return reports"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Fqr8paqTppv"
      },
      "source": [
        "# ==========================================\n",
        "# 8. MAIN EXECUTION EXAMPLE\n",
        "# =========================================="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "tgaRsQA_RVjA",
        "outputId": "77d740df-e354-45a8-96d9-90ae154361cb"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'sanvia' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3377874512.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msanvia\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'sanvia' is not defined"
          ]
        }
      ],
      "source": [
        "sanvia.input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tXfTqLL3TtEo",
        "outputId": "a3a9399a-1c06-4574-8f1e-e2812e45f268"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing report generation on 3 studies...\n",
            "\n",
            "Processing study 1/3\n",
            "\n",
            "============================================================\n",
            "Generating Report for Study: 00568c6b2c47f99e0156c9bca84c3963\n",
            "============================================================\n",
            "üìä Step 1: Extracting deep features...\n",
            "‚úÖ Model prediction successful\n",
            "‚ùå Error: \"Exception encountered when calling Functional.call().\\n\\n\\x1b[1m138492903705824\\x1b[0m\\n\\nArguments received by Functional.call():\\n  ‚Ä¢ inputs={'L_CC': 'tf.Tensor(shape=(1, 768, 768, 3), dtype=float32)', 'L_MLO': 'tf.Tensor(shape=(1, 768, 768, 3), dtype=float32)', 'R_CC': 'tf.Tensor(shape=(1, 768, 768, 3), dtype=float32)', 'R_MLO': 'tf.Tensor(shape=(1, 768, 768, 3), dtype=float32)', 'tabular': 'tf.Tensor(shape=(1, 2), dtype=float32)'}\\n  ‚Ä¢ training=None\\n  ‚Ä¢ mask={'L_CC': 'None', 'L_MLO': 'None', 'R_CC': 'None', 'R_MLO': 'None', 'tabular': 'None'}\\n  ‚Ä¢ kwargs=<class 'inspect._empty'>\"\n",
            "\n",
            "Processing study 2/3\n",
            "\n",
            "============================================================\n",
            "Generating Report for Study: 00dfcde5aaf6cd0aab3c3a0435632b3f\n",
            "============================================================\n",
            "üìä Step 1: Extracting deep features...\n",
            "‚úÖ Model prediction successful\n",
            "‚ùå Error: \"Exception encountered when calling Functional.call().\\n\\n\\x1b[1m138493441546464\\x1b[0m\\n\\nArguments received by Functional.call():\\n  ‚Ä¢ inputs={'L_CC': 'tf.Tensor(shape=(1, 768, 768, 3), dtype=float32)', 'L_MLO': 'tf.Tensor(shape=(1, 768, 768, 3), dtype=float32)', 'R_CC': 'tf.Tensor(shape=(1, 768, 768, 3), dtype=float32)', 'R_MLO': 'tf.Tensor(shape=(1, 768, 768, 3), dtype=float32)', 'tabular': 'tf.Tensor(shape=(1, 2), dtype=float32)'}\\n  ‚Ä¢ training=None\\n  ‚Ä¢ mask={'L_CC': 'None', 'L_MLO': 'None', 'R_CC': 'None', 'R_MLO': 'None', 'tabular': 'None'}\\n  ‚Ä¢ kwargs=<class 'inspect._empty'>\"\n",
            "\n",
            "Processing study 3/3\n",
            "\n",
            "============================================================\n",
            "Generating Report for Study: 01278b9ca2c9a45dac2d38b219c2ad76\n",
            "============================================================\n",
            "üìä Step 1: Extracting deep features...\n",
            "‚úÖ Model prediction successful\n",
            "‚ùå Error: \"Exception encountered when calling Functional.call().\\n\\n\\x1b[1m138493437192544\\x1b[0m\\n\\nArguments received by Functional.call():\\n  ‚Ä¢ inputs={'L_CC': 'tf.Tensor(shape=(1, 768, 768, 3), dtype=float32)', 'L_MLO': 'tf.Tensor(shape=(1, 768, 768, 3), dtype=float32)', 'R_CC': 'tf.Tensor(shape=(1, 768, 768, 3), dtype=float32)', 'R_MLO': 'tf.Tensor(shape=(1, 768, 768, 3), dtype=float32)', 'tabular': 'tf.Tensor(shape=(1, 2), dtype=float32)'}\\n  ‚Ä¢ training=None\\n  ‚Ä¢ mask={'L_CC': 'None', 'L_MLO': 'None', 'R_CC': 'None', 'R_MLO': 'None', 'tabular': 'None'}\\n  ‚Ä¢ kwargs=<class 'inspect._empty'>\"\n",
            "\n",
            "============================================================\n",
            "REPORT GENERATION SUMMARY\n",
            "============================================================\n",
            "‚úÖ Successfully generated 0 reports\n",
            "üìÅ Reports saved in: /content/drive/MyDrive/VnDir_Mammo/sanvia_outputs/generated_reports\n"
          ]
        }
      ],
      "source": [
        "# ==========================================\n",
        "# 8. MAIN EXECUTION EXAMPLE\n",
        "# ==========================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Test on a few studies from validation set\n",
        "    test_studies = view_mapping_df[view_mapping_df['split_final'] == 'val']['study_id'].tolist()[:3]\n",
        "\n",
        "    print(f\"Testing report generation on {len(test_studies)} studies...\")\n",
        "\n",
        "    # Generate reports - pass MODEL_PATH instead of model object\n",
        "    generated_reports = []\n",
        "    for i, study_id in enumerate(test_studies):\n",
        "        print(f\"\\nProcessing study {i+1}/{len(test_studies)}\")\n",
        "        try:\n",
        "            report = generate_comprehensive_report(study_id, view_mapping_df, MODEL_PATH)\n",
        "            generated_reports.append(report)\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error: {e}\")\n",
        "            continue\n",
        "\n",
        "    # Summary\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"REPORT GENERATION SUMMARY\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"‚úÖ Successfully generated {len(generated_reports)} reports\")\n",
        "    print(f\"üìÅ Reports saved in: {OUTPUT_DIR / 'generated_reports'}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
