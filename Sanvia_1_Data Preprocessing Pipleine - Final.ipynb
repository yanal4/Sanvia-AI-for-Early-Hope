{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ogFhyG0WxrSq"
   },
   "source": [
    "# ==========================================\n",
    "# Sanvia - 1 - Data Preprocessing Pipline\n",
    "# =========================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 15042,
     "status": "ok",
     "timestamp": 1768063154667,
     "user": {
      "displayName": "Yanal Alshoufi",
      "userId": "08404563106434208894"
     },
     "user_tz": -180
    },
    "id": "hOegugL0qLf4"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import json\n",
    "import pickle\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import cv2\n",
    "import albumentations as A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "McCampY0tchF"
   },
   "source": [
    "# ==========================================\n",
    "# 1. SEED AND CONFIGURATION\n",
    "# =========================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3241,
     "status": "ok",
     "timestamp": 1768063157954,
     "user": {
      "displayName": "Yanal Alshoufi",
      "userId": "08404563106434208894"
     },
     "user_tz": -180
    },
    "id": "BwcZ2XjeqQAL",
    "outputId": "657ef0db-6763-41e3-83ea-5c4b065d49c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0]\n",
      "TensorFlow: 2.19.0\n",
      "GPU Available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "GPU memory growth enabled for 1 GPU(s)\n",
      "Configuration:\n",
      "{\n",
      "  \"data_dir\": \"/content/drive/MyDrive/VnDir_Mammo\",\n",
      "  \"metadata_csv\": \"/content/drive/MyDrive/VnDir_Mammo/metadata.csv\",\n",
      "  \"breast_annotations_csv\": \"/content/drive/MyDrive/VnDir_Mammo/breast-level_annotations.csv\",\n",
      "  \"finding_annotations_csv\": \"/content/drive/MyDrive/VnDir_Mammo/finding_annotations.csv\",\n",
      "  \"images_dir\": \"/content/drive/MyDrive/VnDir_Mammo/images/images_png\",\n",
      "  \"output_dir\": \"/content/drive/MyDrive/VnDir_Mammo/sanvia_outputs\",\n",
      "  \"scaler_path\": \"/content/drive/MyDrive/VnDir_Mammo/sanvia_outputs/age_scaler.pkl\",\n",
      "  \"class_weights_path\": \"/content/drive/MyDrive/VnDir_Mammo/sanvia_outputs/class_weights.json\",\n",
      "  \"config_path\": \"/content/drive/MyDrive/VnDir_Mammo/sanvia_outputs/config.json\",\n",
      "  \"image_cache_path\": \"/content/drive/MyDrive/VnDir_Mammo/sanvia_outputs/image_paths_cache.json\",\n",
      "  \"valid_fraction\": 0.5,\n",
      "  \"test_split_name\": \"test\",\n",
      "  \"img_size\": [\n",
      "    512,\n",
      "    512\n",
      "  ],\n",
      "  \"num_channels\": 3,\n",
      "  \"dtype\": \"<dtype: 'float32'>\",\n",
      "  \"backbone\": \"EfficientNetB4\",\n",
      "  \"tab_embed_dim\": 128,\n",
      "  \"batch_size\": 16,\n",
      "  \"buffer_size\": 5000,\n",
      "  \"prefetch_buffer\": -1,\n",
      "  \"num_parallel_calls\": -1,\n",
      "  \"cache_dataset\": true,\n",
      "  \"augment_prob\": 0.7,\n",
      "  \"rotation_limit\": 7,\n",
      "  \"brightness_limit\": 0.15,\n",
      "  \"contrast_limit\": 0.15,\n",
      "  \"shift_scale_limit\": 0.1,\n",
      "  \"oversample_factor\": 2.0,\n",
      "  \"birads_classes\": 5,\n",
      "  \"density_classes\": 4,\n",
      "  \"finding_classes\": [\n",
      "    \"mass\",\n",
      "    \"calcification\",\n",
      "    \"architectural_distortion\"\n",
      "  ],\n",
      "  \"focal_gamma\": 2.0,\n",
      "  \"focal_alpha\": 0.25,\n",
      "  \"seed\": 42\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "\n",
    "print(f\"Python: {sys.version}\")\n",
    "print(f\"TensorFlow: {tf.__version__}\")\n",
    "print(f\"GPU Available: {tf.config.list_physical_devices('GPU')}\")\n",
    "\n",
    "# GPU\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"GPU memory growth enabled for {len(gpus)} GPU(s)\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"GPU configuration error: {e}\")\n",
    "\n",
    "DATA_DIR = Path('/content/drive/MyDrive/VnDir_Mammo')\n",
    "\n",
    "plt.rcParams['figure.dpi'] = 150\n",
    "plt.rcParams['savefig.dpi'] = 300\n",
    "\n",
    "CONFIG = {\n",
    "    # Paths\n",
    "    'data_dir': DATA_DIR,\n",
    "    'metadata_csv': DATA_DIR / 'metadata.csv',\n",
    "    'breast_annotations_csv': DATA_DIR / 'breast-level_annotations.csv',\n",
    "    'finding_annotations_csv': DATA_DIR / 'finding_annotations.csv',\n",
    "    'images_dir': DATA_DIR / 'images' / 'images_png',\n",
    "\n",
    "    # Output directories\n",
    "    'output_dir': DATA_DIR / 'sanvia_outputs',\n",
    "    'scaler_path': DATA_DIR / 'sanvia_outputs' / 'age_scaler.pkl',\n",
    "    'class_weights_path': DATA_DIR / 'sanvia_outputs' / 'class_weights.json',\n",
    "    'config_path': DATA_DIR / 'sanvia_outputs' / 'config.json',\n",
    "    'image_cache_path': DATA_DIR / 'sanvia_outputs' / 'image_paths_cache.json',\n",
    "\n",
    "    # Data splits\n",
    "    'valid_fraction': 0.5,\n",
    "    'test_split_name': 'test',\n",
    "\n",
    "    # Image parameters\n",
    "    'img_size': (512, 512),\n",
    "    'num_channels': 3,\n",
    "    'dtype': tf.float32,\n",
    "\n",
    "    # Model parameters\n",
    "    'backbone': 'EfficientNetB4',\n",
    "    'tab_embed_dim': 128,\n",
    "\n",
    "    # Training parameters\n",
    "    'batch_size': 16,\n",
    "    'buffer_size': 5000,\n",
    "    'prefetch_buffer': tf.data.AUTOTUNE,\n",
    "    'num_parallel_calls': tf.data.AUTOTUNE,\n",
    "    'cache_dataset': True,\n",
    "\n",
    "    # Augmentation parameters\n",
    "    'augment_prob': 0.7,\n",
    "    'rotation_limit': 7,\n",
    "    'brightness_limit': 0.15,\n",
    "    'contrast_limit': 0.15,\n",
    "    'shift_scale_limit': 0.1,\n",
    "\n",
    "    # Oversampling\n",
    "    'oversample_factor': 2.0,\n",
    "\n",
    "    # Labels\n",
    "    'birads_classes': 5,\n",
    "    'density_classes': 4,\n",
    "    'finding_classes': ['mass', 'calcification', 'architectural_distortion'],\n",
    "\n",
    "    # Class weights\n",
    "    'focal_gamma': 2.0,\n",
    "    'focal_alpha': 0.25,\n",
    "\n",
    "    'seed': 42,\n",
    "}\n",
    "\n",
    "CONFIG['output_dir'].mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(CONFIG['config_path'], 'w') as f:\n",
    "    json.dump(CONFIG, f, indent=2, default=str, sort_keys=True)\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(json.dumps(CONFIG, indent=2, default=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-kdj1JtNtvY1"
   },
   "source": [
    "# ==========================================\n",
    "# 2. DATA LOADING\n",
    "# =========================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4109,
     "status": "ok",
     "timestamp": 1768063162090,
     "user": {
      "displayName": "Yanal Alshoufi",
      "userId": "08404563106434208894"
     },
     "user_tz": -180
    },
    "id": "ar7DHnYoq27d",
    "outputId": "349bb834-2fb0-47b0-900c-e718c291c264"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata shape: (20000, 21)\n",
      "Breast annotations shape: (20000, 10)\n",
      "Finding annotations shape: (20486, 16)\n"
     ]
    }
   ],
   "source": [
    "def load_vindr_data(config: Dict) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    if not config['metadata_csv'].exists():\n",
    "        raise FileNotFoundError(f\"Metadata CSV not found: {config['metadata_csv']}\")\n",
    "    metadata_df = pd.read_csv(config['metadata_csv'])\n",
    "    print(f\"Metadata shape: {metadata_df.shape}\")\n",
    "\n",
    "    if not config['breast_annotations_csv'].exists():\n",
    "        raise FileNotFoundError(f\"Breast annotations CSV not found: {config['breast_annotations_csv']}\")\n",
    "    breast_df = pd.read_csv(config['breast_annotations_csv'])\n",
    "    print(f\"Breast annotations shape: {breast_df.shape}\")\n",
    "\n",
    "    finding_df = None\n",
    "    if config['finding_annotations_csv'].exists():\n",
    "        finding_df = pd.read_csv(config['finding_annotations_csv'])\n",
    "        print(f\"Finding annotations shape: {finding_df.shape}\")\n",
    "    else:\n",
    "        print(\"Warning: Finding annotations CSV not found.\")\n",
    "\n",
    "    return metadata_df, breast_df, finding_df\n",
    "\n",
    "metadata_df, breast_df, finding_df = load_vindr_data(CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZWE_hW_bt5GX"
   },
   "source": [
    "# ============================================\n",
    "# 3. DATA CLEANING\n",
    "# ============================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 113,
     "status": "ok",
     "timestamp": 1768063162216,
     "user": {
      "displayName": "Yanal Alshoufi",
      "userId": "08404563106434208894"
     },
     "user_tz": -180
    },
    "id": "LYj26nwWq7pT",
    "outputId": "51f5fe0e-108f-4506-84dd-4fc9e0a35207"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: 1 duplicate groups found. No rows will be dropped.\n"
     ]
    }
   ],
   "source": [
    "def clean_and_validate_data(metadata_df: pd.DataFrame,\n",
    "                           breast_df: pd.DataFrame,\n",
    "                           finding_df: Optional[pd.DataFrame]) -> Tuple[pd.DataFrame, pd.DataFrame, Optional[pd.DataFrame]]:\n",
    "\n",
    "    df = breast_df.copy()\n",
    "\n",
    "    required_breast_cols = ['study_id', 'series_id', 'laterality', 'view_position',\n",
    "                           'breast_birads', 'breast_density', 'split']\n",
    "    missing_cols = set(required_breast_cols) - set(df.columns)\n",
    "    if missing_cols:\n",
    "        raise ValueError(f\"Missing required columns: {missing_cols}\")\n",
    "\n",
    "    # Clean BI-RADS - Extract digit, then convert to float to allow NaNs, then subtract 1\n",
    "    df.loc[:, 'breast_birads'] = (\n",
    "        df['breast_birads']\n",
    "        .astype(str)\n",
    "        .str.extract(r'(\\d)')[0] # Returns Series values not found are NaN\n",
    "        .astype(float) - 1\n",
    "    )\n",
    "    birads_invalid = ~df['breast_birads'].isin([0.0, 1.0, 2.0, 3.0, 4.0]) # Check float values\n",
    "    if birads_invalid.any():\n",
    "        print(f\"Warning: {birads_invalid.sum()} invalid BI-RADS values. Dropping.\")\n",
    "        df = df.loc[~birads_invalid].copy() # Ensure new DataFrame is also a copy\n",
    "    df.loc[:, 'breast_birads'] = df['breast_birads'].astype(int) # Now convert to int after dropping NaNs\n",
    "\n",
    "    # Clean density - Extract letter, map to int, convert to float to allow NaNs\n",
    "    df.loc[:, 'breast_density'] = (\n",
    "        df['breast_density']\n",
    "        .astype(str)\n",
    "        .str.extract(r'DENSITY\\s+([A-D])')[0] # Returns Series, values not found are NaN\n",
    "        .map({'A': 0, 'B': 1, 'C': 2, 'D': 3})\n",
    "        .astype(float) # Convert to float to allow NaN values\n",
    "    )\n",
    "    density_invalid = df['breast_density'].isna()\n",
    "    if density_invalid.any():\n",
    "        print(f\"Warning: {density_invalid.sum()} invalid density values. Dropping.\")\n",
    "        df = df.loc[~density_invalid].copy() # Ensure new DataFrame is also a copy\n",
    "    df.loc[:, 'breast_density'] = df['breast_density'].astype(int) # Now convert to int after dropping NaNs\n",
    "\n",
    "    # Detect duplicates but DO NOT drop\n",
    "    duplicates = df.groupby(['study_id', 'laterality', 'view_position']).size()\n",
    "    duplicates = duplicates[duplicates > 1]\n",
    "    if len(duplicates) > 0:\n",
    "        print(f\"Warning: {len(duplicates)} duplicate groups found. No rows will be dropped.\")\n",
    "\n",
    "    # Age column\n",
    "    if \"Patient's Age\" not in metadata_df.columns:\n",
    "        print(\"Warning: 'age' column not found. Creating placeholder.\")\n",
    "        metadata_df['age'] = np.nan\n",
    "\n",
    "    return metadata_df, df, finding_df\n",
    "\n",
    "metadata_df, breast_df, finding_df = clean_and_validate_data(metadata_df, breast_df, finding_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tujPjHgmu3It"
   },
   "source": [
    "# ============================================\n",
    "# 4. DATA DIAGNOSTICS REPORT\n",
    "# ============================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 638,
     "status": "ok",
     "timestamp": 1768063162883,
     "user": {
      "displayName": "Yanal Alshoufi",
      "userId": "08404563106434208894"
     },
     "user_tz": -180
    },
    "id": "zMmybcz7mUud",
    "outputId": "b8058baf-220c-47f1-a7df-7fa666be7919"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1. GENERAL INFORMATION\n",
      "   - Dataset shape: (20000, 10)\n",
      "   - Number of studies: 5000\n",
      "   - Number of unique images: 20000\n",
      "   - Memory usage: 9.32 MB\n",
      "\n",
      " 2. COLUMN DETAILS\n",
      "               DataType  NonNullCount  NullCount  UniqueValues\n",
      "study_id         object         20000          0          5000\n",
      "series_id        object         20000          0          5036\n",
      "image_id         object         20000          0         20000\n",
      "laterality       object         20000          0             2\n",
      "view_position    object         20000          0             2\n",
      "height            int64         20000          0             3\n",
      "width             int64         20000          0            58\n",
      "breast_birads    object         20000          0             5\n",
      "breast_density   object         20000          0             4\n",
      "split            object         20000          0             2\n",
      "\n",
      " 3. BI-RADS CLASS DISTRIBUTION\n",
      "   BI-RADS 1    | Count: 13406 |  67.0%\n",
      "   BI-RADS 2    | Count:  4676 |  23.4%\n",
      "   BI-RADS 3    | Count:   930 |   4.7%\n",
      "   BI-RADS 4    | Count:   762 |   3.8%\n",
      "   BI-RADS 5    | Count:   226 |   1.1%\n",
      "   Total        | Count: 20000 | 100.0%  \n",
      "\n",
      " 4. DENSITY CLASS DISTRIBUTION\n",
      "   DENSITY A    | Count:   100 |   0.5%\n",
      "   DENSITY B    | Count:  1908 |   9.5%\n",
      "   DENSITY C    | Count: 15292 |  76.5%\n",
      "   DENSITY D    | Count:  2700 |  13.5%\n",
      "\n",
      " 5. VIEW POSITION DISTRIBUTION\n",
      "   CC     | Count: 10001 |  50.0%\n",
      "   MLO    | Count:  9999 |  50.0%\n",
      "\n",
      " 6. LATERALITY DISTRIBUTION\n",
      "   L      | Count: 10000 |  50.0%\n",
      "   R      | Count: 10000 |  50.0%\n",
      "\n",
      " 7. SPLIT DISTRIBUTION\n",
      "   training | Count: 16000 |  80.0%\n",
      "   test     | Count:  4000 |  20.0%\n",
      "\n",
      " 8. MISSING VALUES CHECK\n",
      "No missing values found!\n",
      "\n",
      " 9. STUDY STATISTICS\n",
      "   Total studies       : 5000\n",
      "   Avg images per study: 4.00\n",
      "   Min images per study: 4\n",
      "   Max images per study: 4\n",
      "\n",
      " 10. DATA SAMPLE (Cleaned)\n",
      "                           study_id                         series_id                          image_id laterality view_position  height  width breast_birads breast_density     split\n",
      "0  b8d273e8601f348d3664778dae0e7e0b  b36517b9cbbcfd286a7ae04f643af97a  d8125545210c08e1b1793a5af6458ee2          L            CC    3518   2800             1              2  training\n",
      "1  b8d273e8601f348d3664778dae0e7e0b  b36517b9cbbcfd286a7ae04f643af97a  290c658f4e75a3f83ec78a847414297c          L           MLO    3518   2800             1              2  training\n",
      "2  b8d273e8601f348d3664778dae0e7e0b  b36517b9cbbcfd286a7ae04f643af97a  cd0fc7bc53ac632a11643ac4cc91002a          R            CC    3518   2800             1              2  training\n",
      "3  b8d273e8601f348d3664778dae0e7e0b  b36517b9cbbcfd286a7ae04f643af97a  71638b1e853799f227492bfb08a01491          R           MLO    3518   2800             1              2  training\n",
      "4  8269f5971eaca3e5d3772d1796e6bd7a  d931832a0815df082c085b6e09d20aac  dd9ce3288c0773e006a294188aadba8e          L            CC    3518   2800             0              2  training\n",
      "5  8269f5971eaca3e5d3772d1796e6bd7a  d931832a0815df082c085b6e09d20aac  57fbdd278af5c8789a02b355c11620d4          L           MLO    3518   2800             0              2  training\n",
      "6  8269f5971eaca3e5d3772d1796e6bd7a  d931832a0815df082c085b6e09d20aac  202d761a6b0f86faaeefc39ee18b1c53          R            CC    3518   2800             0              2  training\n",
      "7  8269f5971eaca3e5d3772d1796e6bd7a  d931832a0815df082c085b6e09d20aac  acccc1727b61b261844d86aa9de53536          R           MLO    3518   2800             0              2  training\n",
      "8  fa4dcd0f3ba24e86fc8dc25091f7ebd5  a78f4822d806b4f69ba9f0e0c68778b4  a3d0e2394d7db36afab1b6e5e24da798          L            CC    3518   2800             0              2  training\n",
      "9  fa4dcd0f3ba24e86fc8dc25091f7ebd5  a78f4822d806b4f69ba9f0e0c68778b4  48b243704d16570155df12995a284b61          L           MLO    3518   2800             0              2  training\n",
      "\n",
      " 11. KEY OBSERVATIONS\n",
      "    BI-RADS classes: [0, 1, 2, 3, 4]\n",
      "    Density classes: [0, 1, 2, 3]\n",
      "    Unique studies: 5000\n",
      "\n",
      " Diagnostic report saved to: /content/drive/MyDrive/VnDir_Mammo/sanvia_outputs/diagnostic_report.json\n",
      "================================================================================\n",
      " Data diagnostics completed successfully!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n 1. GENERAL INFORMATION\")\n",
    "print(f\"   - Dataset shape: {breast_df.shape}\")\n",
    "print(f\"   - Number of studies: {breast_df['study_id'].nunique()}\")\n",
    "print(f\"   - Number of unique images: {breast_df['image_id'].nunique()}\")\n",
    "print(f\"   - Memory usage: {breast_df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "print(\"\\n 2. COLUMN DETAILS\")\n",
    "column_info = pd.DataFrame({\n",
    "    'DataType': breast_df.dtypes,\n",
    "    'NonNullCount': breast_df.count(),\n",
    "    'NullCount': breast_df.isnull().sum(),\n",
    "    'UniqueValues': breast_df.nunique()\n",
    "})\n",
    "print(column_info)\n",
    "\n",
    "print(\"\\n 3. BI-RADS CLASS DISTRIBUTION\")\n",
    "birads_counts = breast_df['breast_birads'].value_counts().sort_index()\n",
    "for cls, count in birads_counts.items():\n",
    "    percentage = (count / len(breast_df)) * 100\n",
    "    birads_name = f\"BI-RADS {cls + 1}\"\n",
    "    print(f\"   {birads_name:12} | Count: {count:5} | {percentage:5.1f}%\")\n",
    "print(f\"   {'Total':12} | Count: {len(breast_df):5} | {'100.0%':8}\")\n",
    "\n",
    "print(\"\\n 4. DENSITY CLASS DISTRIBUTION\")\n",
    "density_map = {0: 'A', 1: 'B', 2: 'C', 3: 'D'}\n",
    "density_counts = breast_df['breast_density'].value_counts().sort_index()\n",
    "for cls, count in density_counts.items():\n",
    "    percentage = (count / len(breast_df)) * 100\n",
    "    density_name = f\"DENSITY {density_map[cls]}\"\n",
    "    print(f\"   {density_name:12} | Count: {count:5} | {percentage:5.1f}%\")\n",
    "\n",
    "print(\"\\n 5. VIEW POSITION DISTRIBUTION\")\n",
    "view_counts = breast_df['view_position'].value_counts()\n",
    "for view, count in view_counts.items():\n",
    "    percentage = (count / len(breast_df)) * 100\n",
    "    print(f\"   {view:6} | Count: {count:5} | {percentage:5.1f}%\")\n",
    "\n",
    "print(\"\\n 6. LATERALITY DISTRIBUTION\")\n",
    "lat_counts = breast_df['laterality'].value_counts()\n",
    "for lat, count in lat_counts.items():\n",
    "    percentage = (count / len(breast_df)) * 100\n",
    "    print(f\"   {lat:6} | Count: {count:5} | {percentage:5.1f}%\")\n",
    "\n",
    "print(\"\\n 7. SPLIT DISTRIBUTION\")\n",
    "split_counts = breast_df['split'].value_counts()\n",
    "for split, count in split_counts.items():\n",
    "    percentage = (count / len(breast_df)) * 100\n",
    "    print(f\"   {split:8} | Count: {count:5} | {percentage:5.1f}%\")\n",
    "\n",
    "print(\"\\n 8. MISSING VALUES CHECK\")\n",
    "missing_info = breast_df.isnull().sum()\n",
    "if missing_info.sum() == 0:\n",
    "    print(\"No missing values found!\")\n",
    "else:\n",
    "    print(\"Missing values detected:\")\n",
    "    for col, count in missing_info[missing_info > 0].items():\n",
    "        print(f\"   - {col}: {count} missing values\")\n",
    "\n",
    "print(\"\\n 9. STUDY STATISTICS\")\n",
    "studies_stats = {\n",
    "    'Total studies': breast_df['study_id'].nunique(),\n",
    "    'Avg images per study': len(breast_df) / breast_df['study_id'].nunique(),\n",
    "    'Min images per study': breast_df.groupby('study_id').size().min(),\n",
    "    'Max images per study': breast_df.groupby('study_id').size().max()\n",
    "}\n",
    "for stat, value in studies_stats.items():\n",
    "    print(f\"   {stat:20}: {value:.2f}\" if isinstance(value, float) else f\"   {stat:20}: {value}\")\n",
    "\n",
    "print(\"\\n 10. DATA SAMPLE (Cleaned)\")\n",
    "print(breast_df.head(10).to_string())\n",
    "\n",
    "print(\"\\n 11. KEY OBSERVATIONS\")\n",
    "print(f\"    BI-RADS classes: {sorted(breast_df['breast_birads'].unique())}\")\n",
    "print(f\"    Density classes: {sorted(breast_df['breast_density'].unique())}\")\n",
    "print(f\"    Unique studies: {breast_df['study_id'].nunique()}\")\n",
    "\n",
    "diagnostic_report = {\n",
    "    'dataset_shape': breast_df.shape,\n",
    "    'birads_distribution': breast_df['breast_birads'].value_counts().sort_index().to_dict(),\n",
    "    'density_distribution': breast_df['breast_density'].value_counts().sort_index().to_dict(),\n",
    "    'view_distribution': breast_df['view_position'].value_counts().to_dict(),\n",
    "    'laterality_distribution': breast_df['laterality'].value_counts().to_dict(),\n",
    "    'split_distribution': breast_df.get('split_final', breast_df['split']).value_counts().to_dict(),\n",
    "    'missing_values': breast_df.isnull().sum().to_dict(),\n",
    "    'memory_usage_mb': breast_df.memory_usage(deep=True).sum() / 1024**2,\n",
    "    'unique_studies': int(breast_df['study_id'].nunique())\n",
    "}\n",
    "\n",
    "report_path = Path(CONFIG['output_dir']) / 'diagnostic_report.json'\n",
    "with open(report_path, 'w') as f:\n",
    "    json.dump(diagnostic_report, f, indent=2, default=str)\n",
    "\n",
    "print(f\"\\n Diagnostic report saved to: {report_path}\")\n",
    "print(\"=\"*80)\n",
    "print(\" Data diagnostics completed successfully!\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qQhdJvaNvabf"
   },
   "source": [
    "# ============================================\n",
    "# 5. PATIENT SPLITS\n",
    "# ============================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 86,
     "status": "ok",
     "timestamp": 1768063162985,
     "user": {
      "displayName": "Yanal Alshoufi",
      "userId": "08404563106434208894"
     },
     "user_tz": -180
    },
    "id": "6UHqbHvys_8G",
    "outputId": "d3e0c611-182a-4056-bdd8-86313b12cf93"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 4000, Val: 500, Test: 500\n"
     ]
    }
   ],
   "source": [
    "def create_patient_splits(breast_df: pd.DataFrame, config: Dict) -> pd.DataFrame:\n",
    "\n",
    "    studies = breast_df['study_id'].unique()\n",
    "    test_studies = breast_df[breast_df['split'] == config['test_split_name']]['study_id'].unique()\n",
    "    train_studies = np.setdiff1d(studies, test_studies)\n",
    "\n",
    "    valid_studies, final_test_studies = train_test_split(\n",
    "        test_studies,\n",
    "        test_size=1-config['valid_fraction'],\n",
    "        random_state=config['seed']\n",
    "    )\n",
    "\n",
    "    breast_df['split_final'] = 'train'\n",
    "    breast_df.loc[breast_df['study_id'].isin(valid_studies), 'split_final'] = 'val'\n",
    "    breast_df.loc[breast_df['study_id'].isin(final_test_studies), 'split_final'] = 'test'\n",
    "\n",
    "    print(f\"Train: {len(train_studies)}, Val: {len(valid_studies)}, Test: {len(final_test_studies)}\")\n",
    "\n",
    "    return breast_df\n",
    "\n",
    "breast_df = create_patient_splits(breast_df, CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C4UAHdhtvjnQ"
   },
   "source": [
    "# ============================================\n",
    "# 6. MERGE TABULAR FEATURES\n",
    "# ============================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 85,
     "status": "ok",
     "timestamp": 1768063163084,
     "user": {
      "displayName": "Yanal Alshoufi",
      "userId": "08404563106434208894"
     },
     "user_tz": -180
    },
    "id": "dfIup7QKtghe",
    "outputId": "3180fb72-bcb7-43a6-880c-de46765b75eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median age: 45.0\n",
      "Merged dataframe: (20000, 14)\n"
     ]
    }
   ],
   "source": [
    "def merge_tabular_features(breast_df: pd.DataFrame,\n",
    "                           metadata_df: pd.DataFrame,\n",
    "                           config: Dict) -> pd.DataFrame:\n",
    "\n",
    "    if \"Patient's Age\" in metadata_df.columns:\n",
    "        metadata_df = metadata_df.rename(columns={\"Patient's Age\": \"age\"})\n",
    "    elif \"age\" not in metadata_df.columns:\n",
    "        print(\"Warning: No age column found. Creating placeholder.\")\n",
    "        metadata_df[\"age\"] = np.nan\n",
    "\n",
    "    metadata_df[\"age\"] = (\n",
    "        metadata_df[\"age\"]\n",
    "        .astype(str)\n",
    "        .str.extract(r'(\\d+)')\n",
    "        .astype(float)\n",
    "    )\n",
    "\n",
    "    metadata_subset = (\n",
    "        metadata_df[[\"SOP Instance UID\", \"age\"]]\n",
    "        .rename(columns={\"SOP Instance UID\": \"image_name_no_ext\"})\n",
    "        .groupby(\"image_name_no_ext\", as_index=False)\n",
    "        .first()\n",
    "    )\n",
    "\n",
    "    breast_df[\"image_name_no_ext\"] = breast_df[\"image_id\"].str.replace(\".png\", \"\", regex=False)\n",
    "\n",
    "    age_median = metadata_subset[\"age\"].median(skipna=True)\n",
    "    print(f\"Median age: {age_median}\")\n",
    "\n",
    "    metadata_subset[\"age_missing_flag\"] = metadata_subset[\"age\"].isna().astype(int)\n",
    "    metadata_subset[\"age\"] = metadata_subset[\"age\"].fillna(age_median)\n",
    "\n",
    "    merged_df = breast_df.merge(\n",
    "        metadata_subset,\n",
    "        on=\"image_name_no_ext\",\n",
    "        how=\"left\",\n",
    "        validate=\"many_to_one\"\n",
    "    )\n",
    "\n",
    "    if merged_df[\"age\"].isna().any():\n",
    "        merged_df[\"age\"] = merged_df[\"age\"].fillna(age_median)\n",
    "        merged_df[\"age_missing_flag\"] = merged_df[\"age_missing_flag\"].fillna(1).astype(int)\n",
    "\n",
    "    return merged_df\n",
    "\n",
    "merged_df = merge_tabular_features(breast_df, metadata_df, CONFIG)\n",
    "print(f\"Merged dataframe: {merged_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ICRVUrZdvsPT"
   },
   "source": [
    "# ============================================\n",
    "# 7. TABULAR SCALER\n",
    "# ============================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1779,
     "status": "ok",
     "timestamp": 1768063164866,
     "user": {
      "displayName": "Yanal Alshoufi",
      "userId": "08404563106434208894"
     },
     "user_tz": -180
    },
    "id": "Cr1fFTCA3dvg",
    "outputId": "f9ceac44-4f5d-4842-bf60-c33b74a27e91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaler saved to /content/drive/MyDrive/VnDir_Mammo/sanvia_outputs/age_scaler.pkl\n"
     ]
    }
   ],
   "source": [
    "class TabularScaler:\n",
    "    def __init__(self, features: List[str]):\n",
    "        self.features = features\n",
    "        self.scalers = {feat: MinMaxScaler() for feat in features}\n",
    "        self.fitted = False\n",
    "\n",
    "    def fit(self, df: pd.DataFrame):\n",
    "        for feat in self.features:\n",
    "            self.scalers[feat].fit(df[[feat]])\n",
    "        self.fitted = True\n",
    "\n",
    "    def transform(self, df: pd.DataFrame) -> np.ndarray:\n",
    "        if not self.fitted:\n",
    "            raise ValueError(\"Scaler must be fitted first!\")\n",
    "        scaled_features = []\n",
    "        for feat in self.features:\n",
    "            scaled = self.scalers[feat].transform(df[[feat]])\n",
    "            scaled_features.append(scaled)\n",
    "        return np.concatenate(scaled_features, axis=1)\n",
    "\n",
    "    def save(self, path: Path):\n",
    "        scaler_params = {}\n",
    "        for feat, scaler in self.scalers.items():\n",
    "            scaler_params[feat] = {\n",
    "                'min_': scaler.min_.tolist(),\n",
    "                'scale_': scaler.scale_.tolist(),\n",
    "                'data_min_': scaler.data_min_.tolist(),\n",
    "                'data_max_': scaler.data_max_.tolist(),\n",
    "                'data_range_': scaler.data_range_.tolist()\n",
    "            }\n",
    "        with open(path, 'wb') as f:\n",
    "            pickle.dump(scaler_params, f)\n",
    "\n",
    "tabular_features = ['age']\n",
    "tabular_scaler = TabularScaler(tabular_features)\n",
    "\n",
    "train_df = merged_df[merged_df['split_final'] == 'train']\n",
    "tabular_scaler.fit(train_df)\n",
    "\n",
    "merged_df['age_norm'] = tabular_scaler.transform(merged_df)[:, 0]\n",
    "\n",
    "tabular_scaler.save(CONFIG['scaler_path'])\n",
    "print(f\"Scaler saved to {CONFIG['scaler_path']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5_0td2vUvz-M"
   },
   "source": [
    "# ============================================\n",
    "# 8. CREATE VIEW MAPPING WITH ALL CLASSES\n",
    "# ============================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2749,
     "status": "ok",
     "timestamp": 1768063167617,
     "user": {
      "displayName": "Yanal Alshoufi",
      "userId": "08404563106434208894"
     },
     "user_tz": -180
    },
    "id": "xPvHUcz13jP2",
    "outputId": "ce480cc0-9b72-4950-9b34-5bbe89599b95"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " merged_df contains 20000 images with all classes\n",
      "\n",
      " Corrected view mapping distribution:\n",
      "BI-RADS L_CC: breast_birads_L_CC\n",
      "0    3328\n",
      "1    1173\n",
      "2     241\n",
      "3     204\n",
      "4      54\n",
      "Name: count, dtype: int64\n",
      "Density L_CC: breast_density_L_CC\n",
      "0      24\n",
      "1     477\n",
      "2    3821\n",
      "3     678\n",
      "Name: count, dtype: int64\n",
      "\n",
      " Saved corrected mapping: /content/drive/MyDrive/VnDir_Mammo/sanvia_outputs/view_mapping_corrected.csv\n"
     ]
    }
   ],
   "source": [
    "def create_view_mapping(df: pd.DataFrame) -> pd.DataFrame:\n",
    "\n",
    "    result = []\n",
    "\n",
    "    for study_id, group in df.groupby('study_id'):\n",
    "        study_data = {'study_id': study_id}\n",
    "\n",
    "        for _, row in group.iterrows():\n",
    "            view = f\"{row['laterality']}_{row['view_position']}\"\n",
    "            study_data[f'image_id_{view}'] = row['image_id']\n",
    "            study_data[f'breast_birads_{view}'] = row['breast_birads']\n",
    "            study_data[f'breast_density_{view}'] = row['breast_density']\n",
    "            study_data['age_norm'] = row.get('age_norm', 0.5)\n",
    "            study_data['age_missing_flag'] = row.get('age_missing_flag', 0)\n",
    "            study_data['split_final'] = row.get('split_final', 'train')\n",
    "\n",
    "        result.append(study_data)\n",
    "\n",
    "    view_mapping = pd.DataFrame(result)\n",
    "\n",
    "    required_views = ['L_CC', 'L_MLO', 'R_CC', 'R_MLO']\n",
    "    for view in required_views:\n",
    "        if f'image_id_{view}' not in view_mapping.columns:\n",
    "            view_mapping[f'image_id_{view}'] = 'missing.png'\n",
    "            view_mapping[f'breast_birads_{view}'] = 0\n",
    "            view_mapping[f'breast_density_{view}'] = 2\n",
    "\n",
    "    return view_mapping\n",
    "\n",
    "print(f\" merged_df contains {len(merged_df)} images with all classes\")\n",
    "view_mapping = create_view_mapping(merged_df)\n",
    "\n",
    "print(f\"\\n Corrected view mapping distribution:\")\n",
    "print(\"BI-RADS L_CC:\", view_mapping['breast_birads_L_CC'].value_counts().sort_index())\n",
    "print(\"Density L_CC:\", view_mapping['breast_density_L_CC'].value_counts().sort_index())\n",
    "\n",
    "view_mapping.to_csv(CONFIG['output_dir'] / 'view_mapping_corrected.csv', index=False)\n",
    "print(f\"\\n Saved corrected mapping: {CONFIG['output_dir'] / 'view_mapping_corrected.csv'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6SIPM9DqwYJW"
   },
   "source": [
    "# =========================================\n",
    "# 9. UNIFY AGE FIELDS\n",
    "# ========================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 48,
     "status": "ok",
     "timestamp": 1768063167679,
     "user": {
      "displayName": "Yanal Alshoufi",
      "userId": "08404563106434208894"
     },
     "user_tz": -180
    },
    "id": "lBBZFoGU3sUF"
   },
   "outputs": [],
   "source": [
    "def unify_age_fields(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df_copy = df.copy()\n",
    "\n",
    "    age_cols = [c for c in df.columns if 'age_norm_' in c]\n",
    "    flag_cols = [c for c in df.columns if 'age_missing_flag_' in c]\n",
    "\n",
    "    if age_cols:\n",
    "        df_copy['age_norm'] = df_copy[age_cols].mean(axis=1, skipna=True)\n",
    "    if flag_cols:\n",
    "        df_copy['age_missing_flag'] = df_copy[flag_cols].max(axis=1)\n",
    "\n",
    "    return df_copy\n",
    "\n",
    "view_mapping = unify_age_fields(view_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lP8oYvuPwdze"
   },
   "source": [
    "# ============================================\n",
    "# 10. COMPUTE ADVANCED CLASS WEIGHTS\n",
    "# ============================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 739,
     "status": "ok",
     "timestamp": 1768063168421,
     "user": {
      "displayName": "Yanal Alshoufi",
      "userId": "08404563106434208894"
     },
     "user_tz": -180
    },
    "id": "3p4UU6YZU2y4",
    "outputId": "2ad15645-08e1-432f-f408-020986943f14"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " BI-RADS distribution:\n",
      "breast_birads_L_CC\n",
      "0    3328\n",
      "1    1173\n",
      "2     241\n",
      "3     204\n",
      "4      54\n",
      "Name: count, dtype: int64\n",
      "\n",
      "ðŸ“Š Density distribution:\n",
      "breast_density_L_CC\n",
      "0      24\n",
      "1     477\n",
      "2    3821\n",
      "3     678\n",
      "Name: count, dtype: int64\n",
      "\n",
      " Class weights computed:\n",
      "   birads: {'0': 1.0, '1': 2.5576511187579705, '2': 11.888350644466636, '3': 14.01870755276353, '4': 105.12935778082031}\n",
      "   density: {'0': 397.43950968356467, '1': 6.817714903514889, '2': 1.0, '3': 4.844515923073796}\n"
     ]
    }
   ],
   "source": [
    "def compute_class_weights_advanced(df, config, method='effective'):\n",
    "\n",
    "    weights = {}\n",
    "\n",
    "    # BI-RADS\n",
    "    birads_col = 'breast_birads_L_CC'\n",
    "    birads_counts = df[birads_col].value_counts().sort_index()\n",
    "\n",
    "    print(\"\\n BI-RADS distribution:\")\n",
    "    print(birads_counts)\n",
    "\n",
    "    if method == 'effective':\n",
    "        beta = 0.9999\n",
    "        effective_num = 1.0 - np.power(beta, birads_counts)\n",
    "        weights_birads = (1.0 - beta) / effective_num\n",
    "        weights_birads = weights_birads / np.min(weights_birads[birads_counts > 0])\n",
    "        weights_birads[4] *= 2.0\n",
    "\n",
    "    elif method == 'balanced':\n",
    "        total_samples = len(df)\n",
    "        weights_birads = total_samples / (len(birads_counts) * birads_counts)\n",
    "        weights_birads = weights_birads / np.mean(weights_birads)\n",
    "\n",
    "    weights['birads'] = {str(i): float(weights_birads[i]) for i in birads_counts.index}\n",
    "\n",
    "    # Density\n",
    "    density_col = 'breast_density_L_CC'\n",
    "    density_counts = df[density_col].value_counts().sort_index()\n",
    "\n",
    "    print(\"\\n Density distribution:\")\n",
    "    print(density_counts)\n",
    "\n",
    "    if method == 'effective':\n",
    "        effective_num = 1.0 - np.power(beta, density_counts)\n",
    "        weights_density = (1.0 - beta) / effective_num\n",
    "        weights_density = weights_density / np.min(weights_density[density_counts > 0])\n",
    "        weights_density[0] *= 3.0\n",
    "\n",
    "    elif method == 'balanced':\n",
    "        total_samples = len(df)\n",
    "        weights_density = total_samples / (len(density_counts) * density_counts)\n",
    "        weights_density = weights_density / np.mean(weights_density)\n",
    "\n",
    "    weights['density'] = {str(i): float(weights_density[i]) for i in density_counts.index}\n",
    "\n",
    "    weights_path = Path(config['output_dir']) / 'class_weights_advanced.json'\n",
    "    with open(weights_path, 'w') as f:\n",
    "        json.dump(weights, f, indent=2)\n",
    "\n",
    "    print(\"\\n Class weights computed:\")\n",
    "    for task, w in weights.items():\n",
    "        print(f\"   {task}: {w}\")\n",
    "\n",
    "    return weights\n",
    "\n",
    "class_weights = compute_class_weights_advanced(view_mapping, CONFIG, method='effective')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LifHtXAQw7Rq"
   },
   "source": [
    "# ============================================\n",
    "# 11. BUILD IMAGE PATH CACHE\n",
    "# ============================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 1144,
     "status": "ok",
     "timestamp": 1768063169584,
     "user": {
      "displayName": "Yanal Alshoufi",
      "userId": "08404563106434208894"
     },
     "user_tz": -180
    },
    "id": "B_l46wS63-r0"
   },
   "outputs": [],
   "source": [
    "def build_image_path_cache(images_dir: Path, cache_path: Path) -> Dict[str, str]:\n",
    "\n",
    "    if cache_path.exists():\n",
    "        with open(cache_path, 'r') as f:\n",
    "            return json.load(f)\n",
    "\n",
    "    print(\"Building image path cache (first time only)...\")\n",
    "    all_pngs = list(images_dir.rglob(\"*.png\"))\n",
    "    image_map = {p.name: str(p) for p in all_pngs}\n",
    "\n",
    "    with open(cache_path, 'w') as f:\n",
    "        json.dump(image_map, f)\n",
    "\n",
    "    print(f\" Cached {len(image_map)} image paths to {cache_path}\")\n",
    "    return image_map\n",
    "\n",
    "IMAGE_PATH_CACHE = build_image_path_cache(CONFIG['images_dir'], CONFIG['image_cache_path'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hAg445SRxSA1"
   },
   "source": [
    "# ============================================\n",
    "# 12. FINAL SUMMARY\n",
    "# ============================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 640,
     "status": "ok",
     "timestamp": 1768063170242,
     "user": {
      "displayName": "Yanal Alshoufi",
      "userId": "08404563106434208894"
     },
     "user_tz": -180
    },
    "id": "EjoeMypa5FlO",
    "outputId": "a9cff2a3-5b50-41cd-a9f6-51e6b2fced04"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      " OPTIMIZED SANVIA PIPELINE COMPLETE\n",
      "============================================================\n",
      " Image path cache: 20001 images\n",
      " View mapping: 5000 studies\n",
      " Class weights computed\n",
      "============================================================\n",
      " Final view mapping saved to: /content/drive/MyDrive/VnDir_Mammo/sanvia_outputs/view_mapping_final.csv\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\" OPTIMIZED SANVIA PIPELINE COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\" Image path cache: {len(IMAGE_PATH_CACHE)} images\")\n",
    "print(f\" View mapping: {len(view_mapping)} studies\")\n",
    "print(f\" Class weights computed\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "view_mapping.to_csv(CONFIG['output_dir'] / 'view_mapping_final.csv', index=False)\n",
    "print(f\" Final view mapping saved to: {CONFIG['output_dir'] / 'view_mapping_final.csv'}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNrSAwrMtxCl2UnQhBYEuyk",
   "gpuType": "A100",
   "mount_file_id": "1vJdm3c76NsoTEhDtLZ8UE_80jhKEzy0V",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
